# A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models

Transformer를 기반의 사전학습 언어 모델을 사용하는 CTG 기술에 대한 분석을 진행한 논문


## Abstract

- CTG(Controlled Text Generation)는 NLG(Natural Language Generation) 분야에서 새롭게 떠오르는 분야이다.
- 이는 실제 어플리케이션의 제약 조건을 더 잘 충족하고, 보다 자연스러운 텍스트 생성 기술을 개발하는 데 중요한 것으로 여겨진다.
- 최근 PLMs(large-scale pre-trained language models), 특히 transformer 기반의 PLMs을 사용하는 방법이 NLG의 새로운 패러다임으로 자리 잡으면서, 더 다양하고 유창한 텍스트를 생성할 수 있게 되었다.
- 그러나 심층 신경망의 해석 가능성이 낮으므로 이러한 방법의 **제어 가능성을 보장**해야 할 필요성이 있다.
- 근 3-4년 간 다양한 유형의 제약이 필요할 수 있는 여러 CTG task를 대상으로 하는 각기 다른 접근 방식이 등장했다.
- 이 논문에서는, 이 분야의 common task와 주요 접근 방식 그리고 평가 방법에 대하여 체계적인 비판점 검토를 제시한다.

전체 논문 발표 자료: [Paper_240104_CTGSurvey_chaemin.pdf](https://github.com/chaemino/Papers/files/14219154/Paper_240104_CTGSurvey_chaemin.pdf)
